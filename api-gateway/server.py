import json
import os
import shutil
import uuid

import redis
from datetime import datetime
from typing import Optional, List

from fastapi import FastAPI, Depends, HTTPException, Form, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from sqlalchemy.orm import Session
from starlette.responses import FileResponse
from starlette.staticfiles import StaticFiles

from shared import models, schemas
from shared.database import SessionLocal
from shared.models import TaskStatus
from shared.utils.logger import debug_log


app = FastAPI(title="AI Task Gateway", version="2.0.0")

# --- CORS é…ç½® ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Redis è¿æ¥ ---
REDIS_HOST = os.getenv("REDIS_HOST", "127.0.0.1")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
redis_client = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=0)


# --- ä¾èµ–æ³¨å…¥ ---
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


# --- è¾…åŠ©å‡½æ•°ï¼šè·å–æˆ–åˆ›å»ºä¼šè¯ ---
def _get_or_create_conversation(db: Session, conversation_id: Optional[str], prompt: str):
    if conversation_id:
        conv = db.query(models.Conversation).filter(
            models.Conversation.conversation_id == conversation_id
        ).first()
        if conv:
            return conv

    # å¦‚æœæ²¡ä¼  ID æˆ–è€… ID æ²¡æ‰¾åˆ°ï¼Œåˆ›å»ºæ–°çš„
    new_conv_id = conversation_id if conversation_id else str(uuid.uuid4())
    # ç®€å•çš„æ ‡é¢˜ç”Ÿæˆç­–ç•¥ï¼šå– Prompt å‰20ä¸ªå­—
    title = prompt[:20] + "..." if len(prompt) > 20 else prompt

    new_conv = models.Conversation(
        conversation_id=new_conv_id,
        title=title,
        created_at=datetime.now(),
        updated_at=datetime.now()
    )
    db.add(new_conv)
    db.commit()
    db.refresh(new_conv)
    return new_conv


# --- æ ¸å¿ƒé€»è¾‘ï¼šè·¯ç”±åˆ†å‘ ---
def dispatch_to_stream(task_payload: dict) -> str:
    """æ ¹æ®æ¨¡å‹åç§°å†³å®šæŠ•é€’åˆ°å“ªä¸ª Redis Stream"""
    model_name = task_payload.get("model", "").lower()

    stream_key = "gemini_stream"  # é»˜è®¤å…œåº•

    if "qwen" in model_name or "åƒé—®" in model_name:
        stream_key = "qwen_stream"
    elif "deepseek" in model_name:
        stream_key = "deepseek_stream"
    elif "gemini" in model_name:
        stream_key = "gemini_stream"
    elif "sd" in model_name or "stable" in model_name:
        stream_key = "sd_stream"

    # æ‰§è¡ŒæŠ•é€’
    redis_client.xadd(stream_key, {"payload": json.dumps(task_payload)})
    return stream_key


# ==========================================
# API æ¥å£å®šä¹‰
# ==========================================
current_dir = os.path.dirname(os.path.abspath(__file__))
static_dir = os.path.join(current_dir, "static")
if not os.path.exists(static_dir):
    raise RuntimeError(f"âŒ æ‰¾ä¸åˆ°é™æ€ç›®å½•: {static_dir}ï¼Œè¯·ç¡®ä¿å·²åˆ›å»º 'static' æ–‡ä»¶å¤¹å¹¶æ”¾å…¥ index.html")
app.mount("/static", StaticFiles(directory=static_dir), name="static")

current_dir = os.path.dirname(os.path.abspath(__file__))
UPLOAD_DIR = os.path.join(current_dir, "uploads")
os.makedirs(UPLOAD_DIR, exist_ok=True)

@app.get("/")
async def read_root():
    # åŒæ ·ä½¿ç”¨ç»å¯¹è·¯å¾„
    index_file = os.path.join(static_dir, "index.html")
    return FileResponse(index_file)

@app.get("/health")
def health_check():
    return {"status": "ok", "redis": redis_client.ping()}


# === 1. æäº¤ä»»åŠ¡ (Fan-out æ¨¡å¼) ===
@app.post("/v1/chat/completions", response_model=schemas.BatchSubmitResponse)
def create_chat_task(
    # âš ï¸ å¿…é¡»æŠŠåŸæ¥çš„ Pydantic body æ”¹ä¸º Form è¡¨å•å­—æ®µ
    prompt: str = Form(...),
    model: str = Form("gemini-2.5-flash"),
    conversation_id: Optional[str] = Form(None),
    files: List[UploadFile] = File(None),  # âœ¨ æ¥æ”¶æ–‡ä»¶
    db: Session = Depends(get_db)
):
    """
    æ¥æ”¶ç”¨æˆ·è¯·æ±‚ï¼Œåˆ›å»º Batchï¼Œæ‹†åˆ†ä¸ºå¤šä¸ª Task å¹¶åˆ†å‘
    æ”¯æŒ request.model = "gemini-flash, qwen-7b"
    """
    try:
        debug_log("=" * 40, "REQUEST")
        debug_log(f"æ”¶åˆ°è¯·æ±‚ | Models: {model}", "REQUEST")

        saved_file_paths = []
        if files:
            for file in files:
                # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åé˜²æ­¢å†²çª
                file_ext = file.filename.split(".")[-1] if "." in file.filename else "tmp"
                file_name = f"{uuid.uuid4()}.{file_ext}"
                file_path = os.path.join(UPLOAD_DIR, file_name)

                with open(file_path, "wb") as buffer:
                    shutil.copyfileobj(file.file, buffer)

                saved_file_paths.append(file_path)
                debug_log(f"æ–‡ä»¶å·²ä¿å­˜: {file_path}", "INFO")

        # 1. å‡†å¤‡ä¼šè¯
        conversation = _get_or_create_conversation(db, conversation_id, prompt)

        # 2. åˆ›å»º Batch (æ€»è®¢å•)
        new_batch = models.ChatBatch(
            conversation_id=conversation.conversation_id,
            user_prompt=prompt,
            model_config=model,
            status="PROCESSING"
        )

        db.add(new_batch)
        db.commit()
        db.refresh(new_batch)

        # 3. æ‹†åˆ†æ¨¡å‹åˆ—è¡¨ (å»é™¤ç©ºæ ¼)
        # ä¾‹å¦‚: "gemini, qwen" -> ["gemini", "qwen"]
        model_list = [m.strip() for m in model.split(",") if m.strip()]
        if not model_list:
            model_list = ["gemini-2.5-flash"]  # é»˜è®¤å€¼

        created_tasks = []

        # 4. å¾ªç¯åˆ›å»ºå­ä»»åŠ¡
        for model_name in model_list:
            # A. å†™å…¥æ•°æ®åº“
            new_task = models.Task(
                task_id=str(uuid.uuid4()),
                batch_id=new_batch.batch_id,
                conversation_id=conversation.conversation_id,
                prompt=prompt,
                model_name=model_name,
                status=TaskStatus.PENDING,
                task_type="MULTIMODAL" if saved_file_paths else "TEXT",  # æ ‡è®°ç±»å‹
                file_paths=saved_file_paths  # âœ¨ å­˜å…¥æ•°æ®åº“
            )
            db.add(new_task)
            # è¿™é‡Œçš„ commit æ˜¯ä¸ºäº†è®© task_id ç”Ÿæ•ˆï¼Œä¹Ÿå¯ä»¥æ‰¹é‡ commit ä¼˜åŒ–æ€§èƒ½
            db.commit()
            db.refresh(new_task)
            created_tasks.append(new_task)

            # B. ç»„è£… Payload (å‘ç»™ Worker çš„æ•°æ®)
            # Worker ä¸éœ€è¦çŸ¥é“ Batch çš„å­˜åœ¨ï¼Œå®ƒåªè®¤ task_id å’Œ conversation_id
            task_payload = {
                "task_id": new_task.task_id,
                "conversation_id": conversation.conversation_id,
                "prompt": new_task.prompt,
                "model": new_task.model_name,
                "file_paths": saved_file_paths  # âœ¨ ä¼ ç»™ Worker
            }

            # C. å…¥é˜Ÿ Redis
            try:
                target_queue = dispatch_to_stream(task_payload)
                debug_log(f" -> [åˆ†å‘] æ¨¡å‹: {model_name} -> é˜Ÿåˆ—: {target_queue}", "INFO")
            except Exception as e:
                # æ ‡è®°è¯¥å­ä»»åŠ¡å¤±è´¥ï¼Œä½†ä¸å½±å“å…¶ä»–ä»»åŠ¡
                new_task.status = TaskStatus.FAILED
                new_task.error_msg = "ç³»ç»Ÿç¹å¿™: é˜Ÿåˆ—æœåŠ¡å¼‚å¸¸"
                db.commit()

        return {
            "batch_id": new_batch.batch_id,
            "conversation_id": conversation.conversation_id,
            "message": "Tasks dispatched successfully",
            "task_ids": [t.task_id for t in created_tasks]
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Server Error: {str(e)}")


@app.get("/v1/tasks/{task_id}", response_model=schemas.TaskQueryResponse)
def get_task_status(task_id: str, db: Session = Depends(get_db)):
    """
        æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€APIç«¯ç‚¹

        åŠŸèƒ½:
            æ ¹æ®ä»»åŠ¡IDä»æ•°æ®åº“æŸ¥è¯¢ä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬çŠ¶æ€ã€ç»“æœç­‰

        å‚æ•°:
            task_id: è¦æŸ¥è¯¢çš„ä»»åŠ¡IDï¼ˆè·¯å¾„å‚æ•°ï¼‰
            db: æ•°æ®åº“ä¼šè¯ï¼ˆè‡ªåŠ¨æ³¨å…¥ï¼‰

        è¿”å›:
            TaskQueryResponse: åŒ…å«ä»»åŠ¡æ‰€æœ‰è¯¦ç»†ä¿¡æ¯çš„å“åº”

        å¼‚å¸¸:
            HTTP 404: ä»»åŠ¡ä¸å­˜åœ¨
        """
    debug_log(f"æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€: {task_id}", "REQUEST")
    task = db.query(models.Task).filter(models.Task.task_id == task_id).first()
    if not task:
        debug_log(f"ä»»åŠ¡æœªæ‰¾åˆ°: {task_id}", "WARNING")
        raise HTTPException(status_code=404, detail="Task not found")

    debug_log(f"ä»»åŠ¡ {task_id} çŠ¶æ€: {task.status}", "INFO")
    return task




@app.get("/v1/batches/{batch_id}", response_model=schemas.BatchQueryResponse)
def get_batch_result(batch_id: str, db: Session = Depends(get_db)):
    """
    å‰ç«¯è½®è¯¢æ­¤æ¥å£ï¼Œè·å–æ•´ä¸ª Batch çš„æ‰§è¡ŒçŠ¶æ€å’Œæ‰€æœ‰å­æ¨¡å‹çš„ç»“æœ
    """
    batch = db.query(models.ChatBatch).filter(models.ChatBatch.batch_id == batch_id).first()
    if not batch:
        raise HTTPException(status_code=404, detail="Batch ID not found")

    # æ£€æŸ¥æ•´ä½“çŠ¶æ€ (å¯é€‰ä¼˜åŒ–ï¼šå¦‚æœæ‰€æœ‰ Task éƒ½å®Œæˆäº†ï¼Œæ›´æ–° Batch çŠ¶æ€ä¸º COMPLETED)
    # è¿™é‡Œç®€å•å¤„ç†ï¼šç›´æ¥è¿”å› Batch ä¿¡æ¯å’Œå®ƒå…³è”çš„ Tasks

    return {
        "batch_id": batch.batch_id,
        "status": batch.status,
        "user_prompt": batch.user_prompt,
        "created_at": batch.created_at,
        "results": batch.tasks  # SQLAlchemy relationship ä¼šè‡ªåŠ¨æ‹‰å–å­ä»»åŠ¡
    }


@app.get("/v1/conversations/{conversation_id}/history")
def get_history(conversation_id: str, db: Session = Depends(get_db)):
    # 1. è·å–è¯¥ä¼šè¯ä¸‹æ‰€æœ‰æˆåŠŸçš„ä»»åŠ¡ï¼ŒæŒ‰æ—¶é—´æ’åº
    tasks = db.query(models.Task).filter(
        models.Task.conversation_id == conversation_id,
        models.Task.status != TaskStatus.FAILED
    ).order_by(models.Task.created_at.asc()).all()

    # 2. æ„å»ºæ¶ˆæ¯åˆ—è¡¨
    messages = []
    for t in tasks:
        # A. å…ˆæ”¾ç”¨æˆ·çš„æé—®
        messages.append({
            "role": "user",
            "content": t.prompt,
            "model": t.model_name
        })

        # ğŸ”¥ ä¿®æ”¹ï¼šå¦‚æœè¿˜åœ¨è·‘ï¼Œç»™ä¸ªç‰¹æ®Šæ ‡è®°
        if t.status == TaskStatus.SUCCESS and t.response_text:
            messages.append({"role": "assistant", "content": t.response_text, "model": t.model_name})
        elif t.status in [TaskStatus.PENDING, TaskStatus.PROCESSING]:
            # å‘Šè¯‰å‰ç«¯è¿™æ˜¯ä¸€ä¸ªæ­£åœ¨ç”Ÿæˆçš„å ä½ç¬¦
            messages.append({"role": "assistant", "content": "thinking...", "model": t.model_name, "is_loading": True})

    # ç›´æ¥è¿”å›åˆ—è¡¨å³å¯ï¼Œå‰ç«¯é€šå¸¸ç›´æ¥æ¸²æŸ“è¿™ä¸ªæ•°ç»„
    return messages


@app.get("/v1/conversations")
def list_conversations(limit: int = 20, db: Session = Depends(get_db)):
    """
    è·å–æœ€è¿‘çš„ä¼šè¯åˆ—è¡¨ï¼ŒæŒ‰æ›´æ–°æ—¶é—´å€’åºæ’åˆ—
    """
    conversations = db.query(models.Conversation) \
        .order_by(models.Conversation.updated_at.desc()) \
        .limit(limit) \
        .all()

    return {
        "conversations": [
            {
                "conversation_id": c.conversation_id,
                "title": c.title or "æ–°å¯¹è¯",  # å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œæ˜¾ç¤ºé»˜è®¤æ–‡æ¡ˆ
                "modified": c.updated_at
            }
            for c in conversations
        ]
    }


if __name__ == "__main__":
    import uvicorn

    # å¯åŠ¨å‘½ä»¤: python api-gateway/server.py
    uvicorn.run(app, host="0.0.0.0", port=8000)