import json
import os
import time
import socket
from pathlib import Path

import redis
import requests
from datetime import datetime

from dotenv import load_dotenv
from sqlalchemy.orm import Session
from shared import models, database
from shared.models import TaskStatus
from shared.utils import log_error

# --- å…³é”®ä¿®æ”¹ï¼šå¼ºåˆ¶åŠ è½½æ ¹ç›®å½•çš„ .env ---
# è·å–å½“å‰æ–‡ä»¶ (http_worker.py) çš„è·¯å¾„
current_file_path = Path(__file__).resolve()
# å‘ä¸Šæ¨ä¸¤çº§æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½• (workers/gemini/ -> workers/ -> root)
project_root = current_file_path.parent.parent.parent
env_path = project_root / ".env"

if env_path.exists():
    load_dotenv(env_path)
    print(f"âœ… å·²åŠ è½½ç¯å¢ƒå˜é‡: {env_path}")
else:
    print(f"âš ï¸ æœªæ‰¾åˆ°ç¯å¢ƒå˜é‡æ–‡ä»¶: {env_path}")
# -------------------------------------

# --- é…ç½® ---
REDIS_HOST = os.getenv("REDIS_HOST", "127.0.0.1")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
GEMINI_SERVICE_URL = os.getenv("GEMINI_SERVICE_URL", "http://192.168.202.155:61028/v1/chat/completions")
DEBUG = True

# --- Redis Stream é…ç½® ---
STREAM_KEY = "gemini_stream"  # æµåç§° (éœ€ä¸ server.py ä¿æŒä¸€è‡´)
GROUP_NAME = "gemini_workers_group"  # æ¶ˆè´¹è€…ç»„åç§°

worker_identity = os.getenv("WORKER_ID")
if not worker_identity:
    # å…œåº•é€»è¾‘
    worker_identity = f"{socket.gethostname()}-{os.getpid()}"
    print(f"âš ï¸ è­¦å‘Š: æœªæ£€æµ‹åˆ° WORKER_ID ç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨éšæœº ID: {worker_identity}")
CONSUMER_NAME = f"worker-{worker_identity}"

# è¿æ¥ Redis (æ³¨æ„ï¼šdecode_responses=Falseï¼Œå› ä¸º Stream ID æ˜¯ bytes)
redis_client = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=0)


def debug_log(message: str, level: str = "INFO"):
    """ç»Ÿä¸€çš„ debug æ—¥å¿—è¾“å‡º"""
    if DEBUG:
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        emoji_map = {
            "INFO": "â„¹ï¸", "SUCCESS": "âœ…", "ERROR": "âŒ", "WARNING": "âš ï¸",
            "DEBUG": "ğŸ”", "REQUEST": "ğŸ“¥"
        }
        emoji = emoji_map.get(level, "â€¢")
        print(f"[{timestamp}] {emoji} {message}")


def init_stream():
    """åˆå§‹åŒ– Stream å’Œ æ¶ˆè´¹è€…ç»„"""
    try:
        # mkstream=True: å¦‚æœ stream ä¸å­˜åœ¨è‡ªåŠ¨åˆ›å»º
        # id='0': ä»å¤´å¼€å§‹æ¶ˆè´¹ (å¦‚æœæ˜¯ '$' åˆ™åªæ¶ˆè´¹å¯åŠ¨åäº§ç”Ÿçš„æ–°æ¶ˆæ¯)
        redis_client.xgroup_create(STREAM_KEY, GROUP_NAME, id='0', mkstream=True)
        debug_log(f"æ¶ˆè´¹è€…ç»„ {GROUP_NAME} åˆ›å»ºæˆåŠŸ", "INFO")
    except redis.exceptions.ResponseError as e:
        # å¦‚æœç»„å·²ç»å­˜åœ¨ï¼Œä¼šæŠ¥é”™ BUSYGROUPï¼Œå¿½ç•¥å³å¯
        if "BUSYGROUP" in str(e):
            debug_log(f"æ¶ˆè´¹è€…ç»„ {GROUP_NAME} å·²å­˜åœ¨ (æ— éœ€é‡å¤åˆ›å»º)", "INFO")
        else:
            raise e


def process_message(message_id, message_data):
    """
    å¤„ç†å•æ¡æ¶ˆæ¯çš„æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
    """
    db = database.SessionLocal()
    task_id = "UNKNOWN"

    try:
        # 1. è§£ææ•°æ®
        # Stream è¿”å›çš„ message_data ç»“æ„æ˜¯ {b'payload': b'{...}'}
        payload_bytes = message_data.get(b'payload')
        if not payload_bytes:
            debug_log(f"æ¶ˆæ¯æ ¼å¼é”™è¯¯ (ç¼º payload): {message_data}", "ERROR")
            # è„æ•°æ®ç›´æ¥ ACK æ‰ï¼Œå…å¾—æ­»å¾ªç¯
            redis_client.xack(STREAM_KEY, GROUP_NAME, message_id)
            return

        task_data = json.loads(payload_bytes)
        task_id = task_data.get('task_id')
        conversation_id = task_data.get('conversation_id')
        prompt = task_data.get('prompt')
        model = task_data.get('model')

        debug_log(f"å¤„ç†ä»»åŠ¡: {task_id} | æ¨¡å‹: {model}", "REQUEST")

        # 2. æ„é€ è¯·æ±‚å‘é€ç»™ Gemini Service
        payload = {
            "model": model,
            "conversation_id": conversation_id,
            "messages": [{"role": "user", "content": prompt}]
        }

        start_time = time.time()

        # è°ƒç”¨ä¸‹æ¸¸æ¥å£
        response = requests.post(GEMINI_SERVICE_URL, json=payload, timeout=120)

        if response.status_code == 200:
            # === æˆåŠŸé€»è¾‘ ===
            res_json = response.json()
            ai_text = res_json['choices'][0]['message']['content']

            # æ›´æ–°æ•°æ®åº“
            task_record = db.query(models.Task).filter(models.Task.task_id == task_id).first()
            if task_record:
                task_record.response_text = ai_text
                task_record.status = TaskStatus.SUCCESS
                task_record.cost_time = round(time.time() - start_time, 2)

                # æ›´æ–°ä¼šè¯æ´»è·ƒæ—¶é—´
                conv = db.query(models.Conversation).filter(
                    models.Conversation.conversation_id == conversation_id).first()
                if conv:
                    conv.updated_at = datetime.now()

                db.commit()
                debug_log(f"ä»»åŠ¡å®Œæˆ: {task_id} (è€—æ—¶: {task_record.cost_time:.2f}s)", "SUCCESS")

            # 3. å…³é”®ï¼šåªæœ‰ä¸šåŠ¡å¤„ç†æˆåŠŸï¼Œæ‰å‘é€ ACK
            # å‘Šè¯‰ Redisï¼šè¿™æ¡æ¶ˆæ¯ ID å¤„ç†å®Œäº†ï¼Œå¯ä»¥ä» PEL (Pending List) ä¸­ç§»é™¤
            redis_client.xack(STREAM_KEY, GROUP_NAME, message_id)

        else:
            # === å¤±è´¥é€»è¾‘ (API æŠ¥é”™) ===
            error_detail = response.text
            error_msg = f"Gemini Service Error: {response.status_code}"

            debug_log(f"APIè°ƒç”¨å¤±è´¥: {response.status_code}", "ERROR")

            # è®°å½•è¯¦ç»†æ—¥å¿—åˆ°æ•°æ®åº“
            log_error(
                source="Worker-Gemini",
                message=f"APIå“åº”é”™è¯¯: {error_detail[:200]}...",
                task_id=task_id,
                error=Exception(f"HTTP {response.status_code}")
            )

            # æ ‡è®°ä»»åŠ¡å¤±è´¥
            _mark_failed(db, task_id, error_msg)

            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¹Ÿ ACK æ‰ã€‚
            # å› ä¸º API è¿”å› 4xx/500 é€šå¸¸æ˜¯ä¸å¯æ¢å¤çš„ï¼ˆæˆ–è€…éœ€è¦äººå·¥ä»‹å…¥ï¼‰ï¼Œ
            # å¦‚æœä¸ ACKï¼Œå®ƒä¼šä¸€ç›´é‡è¯•ï¼Œå¯èƒ½å¯¼è‡´æ­»å¾ªç¯ã€‚
            # å¦‚æœä½ å¸Œæœ›å®ƒé‡è¯•ï¼Œå¯ä»¥å°†è¿™è¡Œ redis_client.xack(...) æ³¨é‡Šæ‰ã€‚
            redis_client.xack(STREAM_KEY, GROUP_NAME, message_id)

    except (json.JSONDecodeError, UnicodeDecodeError) as e:
        debug_log(f"JSON è§£æå¤±è´¥: {e}", "ERROR")
        # è¿™ç§ä¹Ÿæ˜¯è„æ•°æ®ï¼Œç›´æ¥ ACK ä¸¢å¼ƒ
        redis_client.xack(STREAM_KEY, GROUP_NAME, message_id)

    except requests.exceptions.RequestException as e:
        # === ç½‘ç»œé”™è¯¯ (å¯é‡è¯•) ===
        debug_log(f"è¿æ¥ Gemini Service å¤±è´¥: {e}", "ERROR")
        log_error("Worker-Gemini", "æ— æ³•è¿æ¥ä¸‹æ¸¸æœåŠ¡", task_id, e)
        _mark_failed(db, task_id, "Service Unreachable")
        # è¿™é‡Œ ã€ä¸è¦ã€‘ ACKï¼Œè®©å®ƒç•™åœ¨ Pending List é‡Œ
        # ä¸‹æ¬¡ recover_pending_tasks æˆ–è€…å…¶ä»– Worker å¯ä»¥å†æ¬¡å°è¯•

    except Exception as e:
        # === æœªçŸ¥å†…éƒ¨é”™è¯¯ ===
        debug_log(f"Worker å†…éƒ¨é”™è¯¯: {e}", "ERROR")
        log_error("Worker-Gemini", "Worker å†…éƒ¨é€»è¾‘å¼‚å¸¸", task_id, e)
        _mark_failed(db, task_id, str(e))
        # è¿™ç§é”™è¯¯é€šå¸¸æ˜¯ä»£ç  Bugï¼Œé‡è¯•ä¹Ÿæ²¡ç”¨ï¼Œå»ºè®® ACK æ‰
        redis_client.xack(STREAM_KEY, GROUP_NAME, message_id)

    finally:
        db.close()


def _mark_failed(db, task_id, msg):
    """è¾…åŠ©å‡½æ•°ï¼šæ ‡è®°ä»»åŠ¡å¤±è´¥"""
    try:
        task = db.query(models.Task).filter(models.Task.task_id == task_id).first()
        if task:
            task.status = TaskStatus.FAILED
            task.error_msg = msg
            db.commit()
    except Exception as e:
        db.rollback()
        print(f"âš ï¸ è‡´å‘½é”™è¯¯ï¼šæ— æ³•æ›´æ–°ä»»åŠ¡å¤±è´¥çŠ¶æ€! {e}")


def recover_pending_tasks():
    """
    æ£€æŸ¥å¹¶æ¢å¤ Pending List ä¸­çš„ä»»åŠ¡
    è¿™äº›æ˜¯â€œå·²åˆ†é…ç»™æˆ‘ï¼Œä½†æœª ACKâ€çš„ä»»åŠ¡ï¼ˆé€šå¸¸æ˜¯ä¸Šæ¬¡å´©æºƒæˆ–ç½‘ç»œä¸­æ–­é—ç•™çš„ï¼‰
    """
    # è¿™é‡Œçš„ ID '0' è¡¨ç¤ºè¯»å–æ‰€æœ‰ Pending æ¶ˆæ¯
    # æ—¢ç„¶æ˜¯å•çº¿ç¨‹ Workerï¼Œåªè¦æˆ‘åœ¨ç©ºé—²æ—¶è¯»åˆ°äº† Pending æ¶ˆæ¯ï¼Œè¯´æ˜å®ƒä¸€å®šæ˜¯é—ç•™çš„
    try:
        response = redis_client.xreadgroup(
            GROUP_NAME, CONSUMER_NAME, {STREAM_KEY: '0'}, count=10, block=None
        )

        if response:
            stream_name, messages = response[0]
            if messages:
                debug_log(f"â™»ï¸ å‘ç° {len(messages)} ä¸ªæŒ‚èµ·ä»»åŠ¡ï¼Œæ­£åœ¨é‡è¯•...", "WARNING")
                for message_id, message_data in messages:
                    process_message(message_id, message_data)
                debug_log("âœ… æŒ‚èµ·ä»»åŠ¡é‡è¯•ç»“æŸ", "INFO")
            # å¦‚æœ response ä¸ä¸ºç©ºä½† messages ä¸ºç©ºï¼Œè¯´æ˜æ²¡æœ‰ pending ä»»åŠ¡ï¼Œä¸åšå¤„ç†
    except Exception as e:
        debug_log(f"æ£€æŸ¥æŒ‚èµ·ä»»åŠ¡æ—¶å‡ºé”™: {e}", "ERROR")


def start_worker():
    debug_log("=" * 40, "INFO")
    debug_log(f"Stream Worker å¯åŠ¨: {CONSUMER_NAME}", "INFO")
    debug_log(f"ç›‘å¬æµ: {STREAM_KEY} | ç»„: {GROUP_NAME}", "INFO")

    # 1. åˆå§‹åŒ–
    init_stream()

    # 2. å¯åŠ¨æ—¶å…ˆåšä¸€æ¬¡å…¨é‡æ£€æŸ¥
    recover_pending_tasks()

    debug_log("åˆå§‹åŒ–å®Œæˆï¼Œè¿›å…¥ä¸»å¾ªç¯...", "INFO")

    # === æ–°å¢ï¼šå®šä¹‰å¿ƒè·³æ£€æŸ¥é—´éš” (ç§’) ===
    CHECK_INTERVAL = 60
    last_check_time = time.time()

    # 3. ä¸»å¾ªç¯
    while True:
        try:
            # === æ–°å¢ï¼šå‘¨æœŸæ€§æ£€æŸ¥ Pending List (è¡¥æ¼é€»è¾‘) ===
            current_time = time.time()
            if current_time - last_check_time > CHECK_INTERVAL:
                # åªæœ‰åœ¨ç©ºé—²ï¼ˆèƒ½è·‘åˆ°è¿™é‡Œè¯´æ˜æ²¡è¢«é˜»å¡ï¼‰æ—¶æ‰æ£€æŸ¥
                # è¿™ä¸€æ­¥èƒ½æŠŠä¹‹å‰å› ä¸ºç½‘ç»œé”™è¯¯(RequestsException)è·³è¿‡ ACK çš„ä»»åŠ¡æå›æ¥é‡è¯•
                debug_log("æ‰§è¡Œå‘¨æœŸæ€§å¾…å¤„ç†ä»»åŠ¡æ£€æŸ¥", "INFO")
                recover_pending_tasks()
                last_check_time = current_time
            # ============================================

            # é˜»å¡è¯»å–æ–°æ¶ˆæ¯ (ç‰¹æ®Š ID '>')
            # block=2000 è¡¨ç¤ºé˜»å¡ 2ç§’ã€‚
            # è¿™é‡Œçš„ 2ç§’ ä¹Ÿæ˜¯å¿ƒè·³çš„æœ€å°ç²’åº¦ï¼Œæ„å‘³ç€æœ€å¿« 2ç§’ æ£€æŸ¥ä¸€æ¬¡ï¼Œæœ€æ…¢ (2+å¤„ç†æ—¶é—´)
            response = redis_client.xreadgroup(
                GROUP_NAME, CONSUMER_NAME, {STREAM_KEY: '>'}, count=1, block=2000
            )

            if not response:
                continue

            stream_name, messages = response[0]
            for message_id, message_data in messages:
                process_message(message_id, message_data)

        except Exception as e:
            debug_log(f"Stream å¾ªç¯ä¸¥é‡é”™è¯¯: {e}", "ERROR")
            # è¿™é‡Œä¸ç”¨ log_error æ•°æ®åº“ï¼Œé˜²æ­¢æ­»å¾ªç¯å†™çˆ†æ•°æ®åº“
            time.sleep(5)


if __name__ == "__main__":
    start_worker()