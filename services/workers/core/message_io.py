import json
import time

import redis

from common.logger import debug_log
from common.database import SessionLocal
from common import models
from common.models import TaskStatus

DLQ_STREAM_KEY = "sys_dead_letters"

def send_to_dlq(redis_client, message_id, raw_payload, error_msg, source="Unknown"):
    """
    ğŸ’€ å°†çƒ‚æ¶ˆæ¯ç§»å…¥æ­»ä¿¡é˜Ÿåˆ—ï¼Œå¹¶ ACK ä¸¢å¼ƒ
    """
    try:
        # ç¡®ä¿ message_id æ˜¯å­—ç¬¦ä¸²
        if isinstance(message_id, bytes):
            message_id = message_id.decode()

        # ç¡®ä¿ payload æ˜¯å­—ç¬¦ä¸²
        payload_str = "None"
        if raw_payload:
            payload_str = raw_payload.decode('utf-8', errors='ignore') if isinstance(raw_payload, bytes) else str(
                raw_payload)

        dead_msg = {
            "original_id": message_id,
            "error": str(error_msg),
            "source_worker": source,
            "failed_at": str(int(time.time())),
            "raw_payload": payload_str
        }

        # 1. å…¥æ­»ä¿¡
        redis_client.xadd(DLQ_STREAM_KEY, dead_msg, maxlen=10000)
        debug_log(f"ğŸ’€ å·²ç§»å…¥æ­»ä¿¡é˜Ÿåˆ—: {message_id}", "WARNING")

    except Exception as e:
        debug_log(f"å†™å…¥æ­»ä¿¡é˜Ÿåˆ—å¤±è´¥: {e}", "ERROR")

def parse_and_validate(redis_client, stream_key, group_name, message_id, message_data, consumer_name):
    """
    ğŸ›¡ï¸ é€šç”¨è§£æå‡½æ•°ï¼š
    - å¦‚æœè§£ææˆåŠŸï¼Œè¿”å› task_data (dict)
    - å¦‚æœè§£æå¤±è´¥ï¼ˆJSONé”™è¯¯/ç©ºæ¶ˆæ¯ï¼‰ï¼Œè‡ªåŠ¨å…¥æ­»ä¿¡ + ACKï¼Œå¹¶è¿”å› None
    """
    payload_bytes = message_data.get(b'payload')

    # 1. æ£€æŸ¥ç©ºæ¶ˆæ¯
    if not payload_bytes:
        send_to_dlq(redis_client, message_id, b"", "Empty Payload", consumer_name)
        redis_client.xack(stream_key, group_name, message_id)
        return None

    try:
        # 2. å°è¯•è§£æ JSON
        task_data = json.loads(payload_bytes)
        return task_data

    except (json.JSONDecodeError, UnicodeDecodeError) as e:
        # 3. è§£æå¤±è´¥ -> è‡ªåŠ¨å¤„ç†åäº‹ (DLQ + ACK)
        debug_log(f"æ•°æ®è§£æå¤±è´¥: {e}", "ERROR")
        send_to_dlq(redis_client, message_id, payload_bytes, f"JSON Error: {e}", consumer_name)
        redis_client.xack(stream_key, group_name, message_id)
        return None

def recover_pending_tasks(
        redis_client: redis.Redis,
        stream_key: str,
        group_name: str,
        consumer_name: str,
        process_callback
):
    try:
        # è·å–æ‰€æœ‰å·²è®¤é¢†ä½†æœª ACK çš„æ¶ˆæ¯ (Start from '0')
        response = redis_client.xreadgroup(
            group_name, consumer_name, {stream_key: '0'}, count=50, block=None
        )

        if response:
            stream_name, messages = response[0]
            if messages:
                debug_log(f"â™»ï¸  [{consumer_name}] æ­£åœ¨æ¢å¤ {len(messages)} ä¸ªæŒ‚èµ·ä»»åŠ¡...", "WARNING")

                # è·å–æ•°æ®åº“ä¼šè¯ï¼Œç”¨äºæ‰¹é‡ä¿®å¤çŠ¶æ€
                db = SessionLocal()

                try:
                    for message_id, message_data in messages:
                        # --- 1. å°è¯•è§£æå¹¶ä¿®å¤åƒµå°¸çŠ¶æ€ ---
                        try:
                            # Redis çš„ message_id (å¦‚ "1678888888888-0") å‰åŠéƒ¨åˆ†æ˜¯æ—¶é—´æˆ³(æ¯«ç§’)
                            msg_timestamp = int(message_id.decode().split('-')[0])
                            current_time = int(time.time() * 1000)

                            # å¦‚æœæ¶ˆæ¯è¶…è¿‡ 60 ç§’ï¼ˆå³æ—¶èŠå¤©çš„å®¹å¿åº¦ï¼‰ï¼Œç›´æ¥ä¸¢å¼ƒ
                            if current_time - msg_timestamp > 60000:
                                print(f"â° ä¸¢å¼ƒè¿‡æœŸä»»åŠ¡: {message_id} (è¶…æ—¶ > 60s)")
                                redis_client.xack(stream_key, group_name, message_id)
                                continue  # è·³è¿‡ï¼Œä¸æ‰§è¡Œ

                            payload_bytes = message_data.get(b'payload')
                            if payload_bytes:
                                task_data = json.loads(payload_bytes)
                                task_id = task_data.get('task_id')

                                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¦‚æœä»»åŠ¡çŠ¶æ€æ˜¯ PROCESSINGï¼Œè¯´æ˜æ˜¯ä¸Šæ¬¡å´©æºƒç•™ä¸‹çš„
                                # å¿…é¡»å¼ºåˆ¶é‡ç½®ä¸º PENDINGï¼Œå¦åˆ™åç»­ claim_task ä¼šæŠ¢å å¤±è´¥
                                if task_id:
                                    result = db.query(models.Task).filter(
                                        models.Task.task_id == task_id,
                                        models.Task.status == TaskStatus.PROCESSING
                                    ).update(
                                        {"status": TaskStatus.PENDING},
                                        synchronize_session=False
                                    )
                                    if result > 0:
                                        db.commit()
                                        debug_log(f"ğŸ”§ [è‡ªæ„ˆ] ä¿®å¤åƒµå°¸ä»»åŠ¡: {task_id} PROCESSING -> PENDING", "INFO")

                        except Exception as e:
                            debug_log(f"é¢„æ£€æŸ¥è§£æå¤±è´¥ (å°†ç”± Worker è‡ªåŠ¨å¤„ç†): {e}", "WARNING")
                            # è§£æéƒ½å¤±è´¥äº†ï¼Œé€šå¸¸å»ºè®®ç›´æ¥ ACK è·³è¿‡ï¼Œé˜²æ­¢æ­»å¾ªç¯
                            # redis_client.xack(stream_key, group_name, message_id)
                            # continue

                        # --- 2. è°ƒç”¨å…·ä½“çš„ Worker é€»è¾‘è¿›è¡Œå¤„ç† ---
                        # check_idempotency=True ä¾ç„¶é‡è¦ï¼Œé˜²æ­¢å¤„ç†é‚£äº›å…¶å®å·²ç» SUCCESS ä½†æ²¡ ACK çš„ä»»åŠ¡
                        process_callback(message_id, message_data, check_idempotency=True)

                finally:
                    db.close()

                debug_log("âœ… æŒ‚èµ·ä»»åŠ¡å¤„ç†å®Œæ¯•", "INFO")

    except Exception as e:
        debug_log(f"âŒ æ¢å¤ Pending ä»»åŠ¡æµç¨‹å¤±è´¥: {e}", "ERROR")
